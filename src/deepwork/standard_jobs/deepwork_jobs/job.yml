# yaml-language-server: $schema=../../schemas/job.schema.json
name: deepwork_jobs
version: "0.2.0"
summary: "DeepWork job management commands"
description: |
  Core commands for managing DeepWork jobs. These commands help you define new multi-step
  workflows and learn from running them.

  The `define` command guides you through an interactive process to create a new job by
  asking detailed questions about your workflow, understanding each step's inputs and outputs,
  and generating all necessary files.

  The `learn` command reflects on conversations where DeepWork jobs were run, identifies
  confusion or inefficiencies, and improves job instructions. It also captures bespoke
  learnings specific to the current run into AGENTS.md files in the working folder.

changelog:
  - version: "0.1.0"
    changes: "Initial version"
  - version: "0.2.0"
    changes: "Replaced refine command with learn command for conversation-driven improvement"

steps:
  - id: define
    name: "Define Job Specification"
    description: "Create the job.yml specification file by understanding workflow requirements"
    instructions_file: steps/define.md
    inputs:
      - name: job_purpose
        description: "What complex task or workflow are you trying to accomplish?"
    outputs:
      - job.yml
    dependencies: []
    hooks:
      after_agent:
        - prompt: |
            Verify the job.yml output meets ALL quality criteria before completing:

            1. **User Understanding**: Did you fully understand the user's workflow through interactive Q&A?
            2. **Clear Inputs/Outputs**: Does every step have clearly defined inputs and outputs?
            3. **Logical Dependencies**: Do step dependencies make sense and avoid circular references?
            4. **Concise Summary**: Is the summary under 200 characters and descriptive?
            5. **Rich Description**: Does the description provide enough context for future refinement?
            6. **Valid Schema**: Does the job.yml follow the required schema (name, version, summary, steps)?
            7. **File Created**: Has the job.yml file been created in `.deepwork/jobs/[job_name]/job.yml`?

            If ANY criterion is not met, continue working to address it.
            If ALL criteria are satisfied, include `<promise>✓ Quality Criteria Met</promise>` in your response.

  - id: implement
    name: "Implement Job Steps"
    description: "Generate instruction files for each step based on the job.yml specification"
    instructions_file: steps/implement.md
    inputs:
      - file: job.yml
        from_step: define
    outputs:
      - implementation_summary.md
    dependencies:
      - define
    hooks:
      after_agent:
        - prompt: |
            Verify the implementation meets ALL quality criteria before completing:

            1. **Directory Structure**: Is `.deepwork/jobs/[job_name]/` created correctly?
            2. **Complete Instructions**: Are ALL step instruction files complete (not stubs or placeholders)?
            3. **Specific & Actionable**: Are instructions tailored to each step's purpose, not generic?
            4. **Output Examples**: Does each instruction file show what good output looks like?
            5. **Quality Criteria**: Does each instruction file define quality criteria for its outputs?
            6. **Sync Complete**: Has `deepwork sync` been run successfully?
            7. **Commands Available**: Are the slash-commands generated in `.claude/commands/`?
            8. **Summary Created**: Has `implementation_summary.md` been created?
            9. **Policies Considered**: Have you thought about whether policies would benefit this job?
               - If relevant policies were identified, did you explain them and offer to run `/deepwork_policy.define`?
               - Not every job needs policies - only suggest when genuinely helpful.

            If ANY criterion is not met, continue working to address it.
            If ALL criteria are satisfied, include `<promise>✓ Quality Criteria Met</promise>` in your response.

  - id: learn
    name: "Learn from Job Execution"
    description: "Reflect on conversation to improve job instructions and capture learnings"
    instructions_file: steps/learn.md
    inputs:
      - name: job_name
        description: "Name of the job that was run (optional - will auto-detect from conversation)"
    outputs:
      - learning_summary.md
    dependencies: []
    hooks:
      after_agent:
        - prompt: |
            Verify the learning process meets ALL quality criteria before completing:

            1. **Conversation Analyzed**: Did you review the conversation for DeepWork job executions?
            2. **Confusion Identified**: Did you identify points of confusion, errors, or inefficiencies?
            3. **Instructions Improved**: Were job instructions updated to address identified issues?
            4. **Instructions Concise**: Are instructions free of redundancy and unnecessary verbosity?
            5. **Shared Content Extracted**: Is lengthy/duplicated content extracted into referenced files?
            6. **Bespoke Learnings Captured**: Were run-specific learnings added to AGENTS.md?
            7. **File References Used**: Do AGENTS.md entries reference other files where appropriate?
            8. **Working Folder Correct**: Is AGENTS.md in the correct working folder for the job?
            9. **Generalizable Separated**: Are generalizable improvements in instructions, not AGENTS.md?
            10. **Sync Complete**: Has `deepwork sync` been run if instructions were modified?

            If ANY criterion is not met, continue working to address it.
            If ALL criteria are satisfied, include `<promise>✓ Quality Criteria Met</promise>` in your response.
