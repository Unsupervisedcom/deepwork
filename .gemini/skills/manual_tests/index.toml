# manual_tests
#
# Runs all manual hook/rule tests using sub-agents. Use when validating that DeepWork rules fire correctly.
#
# Generated by DeepWork - do not edit manually

description = "Runs all manual hook/rule tests using sub-agents. Use when validating that DeepWork rules fire correctly."

prompt = """
# manual_tests

**Multi-step workflow**: Runs all manual hook/rule tests using sub-agents. Use when validating that DeepWork rules fire correctly.

> **NOTE**: Gemini CLI requires manual command invocation. After each step, tell the user which command to run next.

A workflow for running manual tests that validate DeepWork rules/hooks fire correctly.

This job tests that rules fire when they should AND do not fire when they shouldn't.
Each test is run in a SUB-AGENT (not the main agent) because:
1. Sub-agents run in isolated contexts where file changes can be detected
2. The Stop hook automatically evaluates rules when each sub-agent completes
3. Sub-agents report results via MAGIC STRINGS that the main agent checks

MAGIC STRING DETECTION: Sub-agents output:
- "TASK_START: <task name>" - ALWAYS at the start of their response
- "HOOK_FIRED: <rule name>" - If a DeepWork hook blocks them
Detection logic:
- TASK_START present + no HOOK_FIRED = hook did NOT fire
- HOOK_FIRED present = hook fired
- Neither present = timeout (hook blocking infinitely)

TIMEOUT PREVENTION: All sub-agent Task calls use max_turns: 5 to prevent
infinite hangs. If a sub-agent hits the limit (e.g., stuck in infinite block),
treat as timeout - PASSED for "should fire" tests, FAILED for "should NOT fire".

TOKEN OVERHEAD: Each sub-agent uses ~16k input tokens (system prompt + tool
definitions). This is unavoidable baseline overhead for agents with Edit access.
Sub-agent prompts include efficiency instructions to minimize additional usage.

CRITICAL: All tests MUST run in sub-agents. The main agent MUST NOT make the file
edits itself - it spawns sub-agents to make edits, then checks the returned magic
strings to determine whether hooks fired.

Steps:
1. run_not_fire_tests - Run all "should NOT fire" tests in PARALLEL sub-agents
2. run_fire_tests - Run all "should fire" tests in SERIAL sub-agents with reverts between

Test types covered:
- Trigger/Safety mode
- Set mode (bidirectional)
- Pair mode (directional)
- Command action
- Multi safety
- Infinite block (prompt and command)
- Created mode (new files only)


## Available Steps

1. **run_not_fire_tests** - Runs all 'should NOT fire' tests in parallel sub-agents. Use to verify rules don't fire when safety conditions are met.
   Command: `/manual_tests:run_not_fire_tests`
2. **run_fire_tests** - Runs all 'should fire' tests serially with git reverts between each. Use after NOT-fire tests to verify rules fire correctly. (requires: run_not_fire_tests)
   Command: `/manual_tests:run_fire_tests`

## Execution Instructions

### Step 1: Analyze Intent

Parse any text following `/manual_tests` to determine user intent:
- "run_not_fire_tests" or related terms → start at `/manual_tests:run_not_fire_tests`
- "run_fire_tests" or related terms → start at `/manual_tests:run_fire_tests`

### Step 2: Direct User to Starting Step

Tell the user which command to run:
```
/manual_tests:run_not_fire_tests
```

### Step 3: Guide Through Workflow

After each step completes, tell the user the next command to run until workflow is complete.

### Handling Ambiguous Intent

If user intent is unclear:
- Present available steps as numbered options
- Ask user to select the starting point

## Reference

- Job definition: `.deepwork/jobs/manual_tests/job.yml`
"""